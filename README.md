# distilBERT-Student-Layers-Initialization
 This project explores the effectiveness of knowledge distillation across various initialization strategies for student models, derived from a more complex teacher model. Through systematic experimentation, we assess how initializing student models with different layers (top, bottom, odd, even) from a pre-trained teacher model affects their learning capability and performance on a given task.
